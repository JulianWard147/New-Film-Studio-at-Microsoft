{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Microsoft's New Movie Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to explore the relationship between the movie success and movie time of release,movie runtime, movie genre, directors, and actors/actresses. \n",
    "Success will be based on ROI and Profit margin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First object is to clean up the movie budget dataframe, then merge all the relevant data sets so we have one dataframe\n",
    "that is easy to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Complete cell to create money metrics df for merging\n",
    "#importing stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load all the data.\n",
    "bom_movie_gross_df = pd.read_csv('zippedData/bom.movie_gross.csv.gz')\n",
    "imdb_name_basics_df = pd.read_csv('zippedData/imdb.name.basics.csv.gz')\n",
    "imdb_title_akas_df = pd.read_csv('zippedData/imdb.title.akas.csv.gz')\n",
    "imdb_title_basics_df = pd.read_csv('zippedData/imdb.title.basics.csv.gz')\n",
    "imdb_title_crew_df = pd.read_csv('zippedData/imdb.title.crew.csv.gz')\n",
    "imdb_title_principals_df = pd.read_csv('zippedData/imdb.title.principals.csv.gz')\n",
    "imdb_title_ratings_df = pd.read_csv('zippedData/imdb.title.ratings.csv.gz')\n",
    "rt_movie_info_df = pd.read_csv('zippedData/rt.movie_info.tsv.gz', delimiter='\\t')\n",
    "rt_reviews_df = pd.read_csv('zippedData/rt.reviews.tsv.gz', delimiter='\\t', encoding='latin-1')\n",
    "tmdb_movies_df = pd.read_csv('zippedData/tmdb.movies.csv.gz')\n",
    "tn_movie_budgets_df = pd.read_csv('zippedData/tn.movie_budgets.csv.gz')\n",
    "\n",
    "#renaming it to work on\n",
    "money_metrics_df = tn_movie_budgets_df.copy()\n",
    "\n",
    "#funtion for cleaning\n",
    "def clean_a_money_column(df, series_title):\n",
    "    df[series_title] = df[series_title].str.replace('$','')\n",
    "    df[series_title] = df[series_title].str.replace(',','')\n",
    "    df[series_title] = pd.to_numeric(df[series_title])\n",
    "    return df\n",
    "\n",
    "#cleaning\n",
    "money_metrics_df = clean_a_money_column(money_metrics_df,'worldwide_gross')\n",
    "money_metrics_df = clean_a_money_column(money_metrics_df,'domestic_gross')\n",
    "money_metrics_df = clean_a_money_column(money_metrics_df,'production_budget')\n",
    "\n",
    "#unique ID for later merging madness\n",
    "money_metrics_df['mmdf_uni_id'] = money_metrics_df.index\n",
    "\n",
    "#adding profit margin and ROI columns\n",
    "numerator = money_metrics_df['worldwide_gross'] - money_metrics_df['production_budget']\n",
    "money_metrics_df['Profit Margin'] = (numerator*100)/money_metrics_df['worldwide_gross']\n",
    "money_metrics_df.loc[money_metrics_df['Profit Margin'] == float('-inf'), 'Profit Margin'] = 0\n",
    "money_metrics_df['ROI'] = (money_metrics_df['worldwide_gross']/money_metrics_df['production_budget'])\n",
    "\n",
    "# This line of code messes with my data cleaning - Vu\n",
    "#setting profit margin desc as order for the table\n",
    "# money_metrics_df=money_metrics_df.sort_values(['Profit Margin'], ascending=False)\n",
    "\n",
    "#creating year and month columns\n",
    "#Vu's magic lambda, plus to numeric on the year \n",
    "money_metrics_df['Year'] = pd.to_numeric(money_metrics_df['release_date'].map(lambda x: x[-4:]))\n",
    "money_metrics_df['Month'] = money_metrics_df['release_date'].map(lambda x: x[:3])\n",
    "\n",
    "#vu's dupe check\n",
    "movie_dupes = pd.Series(money_metrics_df['movie'].value_counts())\n",
    "title_dupes = [movie_dupes.index[index] for index in list(range(len(movie_dupes))) if movie_dupes.values[index] > 1]\n",
    "\n",
    "#making a new df to do the rename in\n",
    "title_dupes_df = money_metrics_df[money_metrics_df['movie'].isin(title_dupes)].copy()\n",
    "\n",
    "#Making the new title by combining the name of the movie and the year of release\n",
    "title_dupes_df['New Title'] = title_dupes_df['movie'] + ' ' + title_dupes_df['Year'].astype(str)\n",
    "\n",
    "#creating a new DF dropping all the columns we don't need \n",
    "title_dupes_df_merger = title_dupes_df[['mmdf_uni_id', 'New Title']]\n",
    "#Don't actually know if we need this but, whatever.\n",
    "money_metrics_df.reset_index(inplace = True)\n",
    "\n",
    "#reintegrating new titles in a new df mergedf\n",
    "mergedf = money_metrics_df.merge(title_dupes_df_merger, on = 'mmdf_uni_id', how= 'left')\n",
    "\n",
    "#vu's slightly less magical de-duplicator\n",
    "#new column for boolean\n",
    "mergedf['is_title_NaN'] = mergedf['New Title'].isnull()\n",
    "#for loop to rename 'Movie'\n",
    "for movie_index in list(range(len(mergedf.index))):\n",
    "    if  ~mergedf['is_title_NaN'][movie_index]:\n",
    "        mergedf['movie'][movie_index] = mergedf['New Title'][movie_index]\n",
    "\n",
    "money_metrics_merge_ready_df = mergedf.copy()\n",
    "\n",
    "#### NOW OUTPUTS A DF CALLED money_metrics_merge_ready\n",
    "#### TRY TO NOT MESS WITH THIS BLOCK, MAKE A COPY OF money_metrics_merge_ready\n",
    "#### AND MESS WITH THAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genres Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in IMDB's title.basics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df = imdb_title_basics_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicates in IMDB's title.basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_dupes(series):\n",
    "    series_vcs = pd.Series(series.value_counts())\n",
    "    series_dupes = [series_vcs.index[index] for index in list(range(len(series_vcs))) if series_vcs.values[index] > 1]\n",
    "    print(\"Amount of unique duplicate movie title: \" + str(len(series_dupes)))\n",
    "    print(\"Total amount of duplicate movie titles: \" + str(series_vcs[0:len(series_dupes)].sum()))\n",
    "    \n",
    "    return series_vcs\n",
    "\n",
    "# How many duplicates are in the 'primary_title' column?\n",
    "print(determine_dupes(g_df['primary_title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 'title_dupes_df' DataFrame to include new movie titles with movie name and movie year for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique ID column within g_df so that merge with title_dupes_df goes smoothly later.\n",
    "g_df.reset_index(inplace=True)\n",
    "g_df.rename(columns={'index' : 'g_unique_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of all the duplicate movie titles.\n",
    "title_vcs = pd.Series(g_df['primary_title'].value_counts())\n",
    "title_dupes = [title_vcs.index[index] for index in list(range(len(title_vcs))) if title_vcs.values[index] > 1]\n",
    "title_dupes_df = g_df[g_df['primary_title'].isin(title_dupes)].copy()\n",
    "\n",
    "# Create a new column called 'updated_title' that includes the name of the movie and the year of the movie.\n",
    "title_dupes_df['updated_title'] = title_dupes_df['primary_title'] + ' ' + title_dupes_df['start_year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique duplicates are there in `title_dupes_df`?\n",
    "# How does this compare to unique duplicates that were in `g_df`?\n",
    "print(determine_dupes(title_dupes_df['updated_title']))\n",
    "# Over 66% of the records that once weren't unique are now unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that shouldn't be around during the merge (for duplication purposes).\n",
    "remove_columns = ['tconst', 'primary_title', 'original_title', 'start_year', 'runtime_minutes', 'genres']\n",
    "title_dupes_df.drop(columns=remove_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging `'title_dupes_df1'` with `'g_df'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_and_dupes_df = pd.merge(left=g_df, right=title_dupes_df, on=['g_unique_id'], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16GB RAM - THIS CODE BLOCK WILL TAKE ~1 MINUTE TO FINISH RUNNING\n",
    "# It has to iterate through 140,000+ indices...\n",
    "\n",
    "# Assign non-null values in the 'updated_title' column to their respective 'primary_title'\n",
    "# Found it easiest to create a new column that says whether or not a non-null value is in \n",
    "# the 'updated_title' column.\n",
    "g_and_dupes_df['is_title_NaN'] = g_and_dupes_df['updated_title'].isnull()\n",
    "for movie_index in list(range(len(g_and_dupes_df.index))):\n",
    "    if ~g_and_dupes_df.loc[movie_index, 'is_title_NaN']:\n",
    "        g_and_dupes_df.loc[movie_index, 'primary_title'] = g_and_dupes_df.loc[movie_index, 'updated_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns that we don't need for further data exploration.\n",
    "remove_columns = ['g_unique_id','updated_title', 'is_title_NaN']\n",
    "g_and_dupes_df.drop(columns=remove_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign g_and_dupes_df to g_df.\n",
    "g_df = g_and_dupes_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging The Numbers' movie_budgets with IMDB's title.basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the movie title column, 'primary_title', within IMDB's title.basics, to make the\n",
    "# merges a little easier down the road.\n",
    "g_df.rename(columns={'primary_title' : 'movie'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reassign money_metrics_merge_ready_df to mm_df and re-organize/re-label columns w/in mm_df.\n",
    "mm_df = money_metrics_merge_ready_df.copy()\n",
    "mm_df.rename(columns={'Profit Margin': 'profit_margin', 'Year': 'year'}, inplace=True)\n",
    "remove_columns = ['index', 'id','Month', 'New Title', 'is_title_NaN', 'mmdf_uni_id']\n",
    "mm_df.drop(columns=remove_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join g_df into mm_df (NOTE: mm_df same as money_metrics_merge_ready_df).\n",
    "mm_and_g_df = pd.merge(left=mm_df, right=g_df, on='movie', how ='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional merge is necessary because some of the movie titles in `mm_df` don't have duplicates and didn't need to be renamed with a year. Some of the values that just got merged from `g_df` didn't associate to a record in `mm_df` because it was renamed with a year and couldn't find it's match in `mm_df`. To fix this, we'll merge an unaltered copy of the original `imdb_title_basics_df` with a copy of `mm_and_g_df`. With that being said, we'll introduce more duplicates, but it will be manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copies of mm_and_g_df and imdb_title_basics_df for the merge.\n",
    "unaltered_g_df = imdb_title_basics_df.copy()\n",
    "unaltered_g_df.rename(columns={'primary_title' : 'movie'}, inplace=True)\n",
    "mm_and_g_copy_df = mm_and_g_df.copy()\n",
    "\n",
    "# Left join unaltered_g_df into mm_and_g_copy_df.\n",
    "mm_and_g_df = pd.merge(left=mm_and_g_copy_df, right=unaltered_g_df, on='movie', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tconst has a null value on mm_and_g_copy_df side AND\n",
    "# tcont has a value on the unaltered_g_df side AND\n",
    "# the years from both DataFrames match up...\n",
    "# THEN proceed to update all the apprropriate columns on the mm_and_g_copy_df side.\n",
    "# Appropriate being, any columns on the mm_and_g_copy_df side that have null values.\n",
    "mm_and_g_df['is_tconst_x_NaN'] = mm_and_g_df['tconst_x'].isnull()\n",
    "mm_and_g_df['is_tconst_y_NaN'] = mm_and_g_df['tconst_y'].isnull()\n",
    "\n",
    "for movie_index in list(range(len(mm_and_g_df.index))):\n",
    "    if (mm_and_g_df.loc[movie_index, 'is_tconst_x_NaN']) and (~mm_and_g_df.loc[movie_index, 'is_tconst_y_NaN'])\\\n",
    "        and (mm_and_g_df.loc[movie_index, 'start_year_y'].astype('int64') == mm_and_g_df.loc[movie_index, 'year']):\n",
    "            mm_and_g_df.loc[movie_index, 'tconst_x'] = mm_and_g_df.loc[movie_index, 'tconst_y']\n",
    "            mm_and_g_df.loc[movie_index, 'original_title_x'] = mm_and_g_df.loc[movie_index, 'original_title_y']\n",
    "            mm_and_g_df.loc[movie_index, 'start_year_x'] = mm_and_g_df.loc[movie_index, 'start_year_y']\n",
    "            mm_and_g_df.loc[movie_index, 'runtime_minutes_x'] = mm_and_g_df.loc[movie_index, 'runtime_minutes_y']\n",
    "            mm_and_g_df.loc[movie_index, 'genres_x'] = mm_and_g_df.loc[movie_index, 'genres_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize/re-label columns w/in mm_and_g_df.\n",
    "remove_columns = ['is_tconst_x_NaN', 'is_tconst_y_NaN', 'tconst_y', 'original_title_y', \n",
    "                  'start_year_y', 'runtime_minutes_y', 'genres_y']\n",
    "mm_and_g_df.drop(columns=remove_columns, inplace=True)\n",
    "\n",
    "mm_and_g_df.rename(columns={'tconst_x': 'tconst', 'original_title_x': 'original_title',\n",
    "                                 'start_year_x': 'start_year', 'runtime_minutes_x': 'runtime_minutes',\n",
    "                                 'genres_x': 'genres'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records in mm_and_g_df have null values in the tconst column?\n",
    "mm_and_g_null_values = mm_and_g_df['tconst'].isna().sum()\n",
    "print(f'Rows with Null Values in tconst Column: {mm_and_g_null_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where tconst has a null value.\n",
    "mm_and_g_df.dropna(subset=['tconst'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging is complete!\n",
    "\n",
    "## Manual Data Cleaning...\n",
    "\n",
    "First off, Avatar is not a Horror film... It is an Action, Adventure, Fantasy. So let's first change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_and_g_df.loc[mm_and_g_df['movie'] == 'Avatar', 'genres'] = \"Action,Adventure,Fantasy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to manually clean up any duplicates at this point as I have exhausted all of my options for removing duplicates with the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique ID column in the newly merged DataFrame to assist with manual data cleaning.\n",
    "mm_and_g_df.reset_index(inplace=True)\n",
    "mm_and_g_df.rename(columns={'index' : 'unique_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many duplicates do we manually need to remove?\n",
    "determine_dupes(mm_and_g_df['movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that will be used to help automate the manual removal process.\n",
    "def generate_uid_removal_list(test_df, keep_uid_list):\n",
    "    test_df = test_df.drop_duplicates(subset=['unique_id'])\n",
    "    for uid in keep_uid_list:\n",
    "        test_df = test_df[test_df['unique_id'] != uid]\n",
    "\n",
    "    return test_df['unique_id'].tolist()\n",
    "\n",
    "def remove_uid_from_df(df, uid_removal_list):\n",
    "    for uid in uid_removal_list:\n",
    "        df = df[df['unique_id'] != uid]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def individual_manual_data_cleaning(df, movie_title, keep_uid_list):\n",
    "    # print(\"Shape of df before cleaning:\", df.shape)\n",
    "    test_df = df[df['movie'] == movie_title]\n",
    "    uid_removal_list = generate_uid_removal_list(test_df, keep_uid_list)\n",
    "    df = remove_uid_from_df(df, uid_removal_list)\n",
    "    # print(\"Shape of df after cleaning:\", df.shape)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def overall_manual_data_cleaning(df, keep_uid_tuples_list):\n",
    "    for keep_uid_tuple in keep_uid_tuples_list:\n",
    "        # display(df[df['movie'] == keep_uid_tuple[0]])\n",
    "        df = individual_manual_data_cleaning(df, keep_uid_tuple[0], keep_uid_tuple[1])\n",
    "        # display(df[df['movie'] == keep_uid_tuple[0]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_uid_tuples_list = [('A Better Life', [4379]),\n",
    "                        ('Abduction', [1982]),\n",
    "                        ('Absentia', [6881]),\n",
    "                        ('Addicted', [5139]),\n",
    "                        ('After', [6498]),\n",
    "                        ('Alice in Wonderland 2010', [51]),\n",
    "                        ('Believe', [5530]),\n",
    "                        ('Big Eyes', [4312]),\n",
    "                        ('Brotherly Love', [6031]),\n",
    "                        ('Brothers 2015', [3946]),\n",
    "                        ('Burlesque', [1167]),\n",
    "                        ('Cinderella 2015', [451]),\n",
    "                        ('Coco', [113]),\n",
    "                        ('Crossroads 2015', [6613]),\n",
    "                        ('Cyrus', [4806]),\n",
    "                        ('Denial', [4360]),\n",
    "                        ('Destiny', [6470]),\n",
    "                        ('Exeter', [6927]),\n",
    "                        ('Heist 2015', [4561]),\n",
    "                        ('Hercules 2014', [418]),\n",
    "                        ('Highway', [6005]),\n",
    "                        ('Home 2015', [265]),\n",
    "                        ('Homefront', [2877]),\n",
    "                        ('Hush', [6399]),\n",
    "                        ('Joe', []),\n",
    "                        ('Leap Year', [3230]),\n",
    "                        ('Let There Be Light', [5645]),\n",
    "                        ('Lights Out', [5080]),\n",
    "                        ('Phantom', [3357]),\n",
    "                        ('Redemption', [2832]),\n",
    "                        ('Robin Hood 2018', [446]),\n",
    "                        ('Sisters', [2186]),\n",
    "                        ('Spotlight', [2997]),\n",
    "                        ('Stronger', [2351]),\n",
    "                        ('The Artist', [3470]),\n",
    "                        ('The Bounty Hunter', [1488]),\n",
    "                        ('The Call', [3953]),\n",
    "                        ('The Circle', [3310]),\n",
    "                        ('The Darkness', [5389]),\n",
    "                        ('The Family', [2264]),\n",
    "                        ('The Forest', [4275]),\n",
    "                        ('The Night Before', [2605]),\n",
    "                        ('The Prince', [3372]),\n",
    "                        ('The Promise', []),\n",
    "                        ('The Square 2013', [6136]),\n",
    "                        ('The Tempest', [3185]),\n",
    "                        ('The Walk', [2040]),\n",
    "                        ('The Wall', [5684]),\n",
    "                        ('Treachery', [6506]),\n",
    "                        ('Truth or Dare', [5503]),\n",
    "                        ('Underdogs', [5819]),\n",
    "                        ('Weekend', [6807])\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_and_g_df = overall_manual_data_cleaning(mm_and_g_df, keep_uid_tuples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANY MORE DUPES?!?\n",
    "\n",
    "Nope..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_dupes(mm_and_g_df['movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do with NaN Values in `'genres'` column??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows have null values in the 'genres' column?\n",
    "mm_and_g_null_values = mm_and_g_df['genres'].isna().sum()\n",
    "print(f'Rows with Null Values in genres Column: {mm_and_g_null_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows wih null values in the 'genres' column.\n",
    "mm_and_g_df.dropna(subset=['genres'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dtype of `'genres'` from `'string'` to `'list'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_and_g_df['genres'] = mm_and_g_df['genres'].map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for genre data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary with all of the unique genre types from a DataFrame.\n",
    "\n",
    "def unique_g(g_series):\n",
    "    unique_g_set = set()\n",
    "    unique_g_list = []\n",
    "    unique_g_dict = {}\n",
    "    \n",
    "    unique_g_set = set([g for g_list in g_series for g in g_list])\n",
    "    unique_g_list = sorted(list(unique_g_set))\n",
    "    unique_g_dict = {k: v for k, v in enumerate(unique_g_list)}\n",
    "        \n",
    "    return(unique_g_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a DataFrame with records pertaining only to a specified genre.\n",
    "# In addition, the returned DataFrame will have a new genre column\n",
    "# categorizing each record with the specified genre.\n",
    "\n",
    "def movies_w_specified_genres(g_df, g_type):\n",
    "    movies_for_removal = []\n",
    "     \n",
    "    for movie_index in list(range(len(g_df.index))):\n",
    "        test_df = pd.DataFrame(g_df.iloc[movie_index]).T\n",
    "        if g_type not in test_df['genres'][test_df.index[0]]:\n",
    "            movies_for_removal.append(test_df['tconst'][test_df.index[0]])\n",
    "            \n",
    "    for movie_for_removal in movies_for_removal:\n",
    "         g_df = g_df[g_df['tconst'] != movie_for_removal]\n",
    "    \n",
    "    g_df['genre'] = g_type\n",
    "    \n",
    "    return(g_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a concatenated DataFrame of all of unique genres with genre labels in the genre column.\n",
    "\n",
    "def concatenate_all_genres(df, unique_g_dict):\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    for g_index in list(range(len(unique_g_dict))):\n",
    "        temp_df = temp_df.append(movies_w_specified_genres(df, unique_g_dict[g_index]))\n",
    "    \n",
    "    return(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_g_dict = unique_g(mm_and_g_df['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16GB RAM - THIS CODE BLOCK WILL TAKE ~1 MINUTE TO FINISH RUNNING\n",
    "all_g_df = concatenate_all_genres(mm_and_g_df, unique_g_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot bar charts comparing mean ROI vs. Genre and mean Production Budget vs. Genre for all films released in the past decade. We don't want to include films released too long ago as those won't show recent trends with film genres. Also, mean should be used as the measure of central tendency for both plots because we should embrace and introduce \"big hit\" movies, aka outliers, into the visualization since there tends too be a lot of uncertainty in show biz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = all_g_df.loc[all_g_df['year'] > 2010].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ROI_by_g_df = viz_df.groupby('genre').mean()\n",
    "mean_ROI_by_g_df.sort_values('ROI', inplace=True, ascending=False)\n",
    "\n",
    "mean_prod_budget_by_g_df = viz_df.groupby('genre').mean()\n",
    "mean_prod_budget_by_g_df.sort_values('production_budget', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(18,15))\n",
    "\n",
    "colors_ax1 = ['darkgreen', 'navy', 'darkgoldenrod', 'darkgrey', 'darkgrey', 'darkgrey', \n",
    "              'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey',\n",
    "              'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey',\n",
    "              'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey']\n",
    "ax1.bar(x=mean_ROI_by_g_df.index, height=mean_ROI_by_g_df['ROI'], color=colors_ax1)\n",
    "ax1.set_xlabel('Genre')\n",
    "ax1.set_ylabel('ROI (%)')\n",
    "ax1.set_title('Mean ROI by Genre for Films Released in the Past Decade')\n",
    "\n",
    "colors_ax2 = ['darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey',\n",
    "              'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgrey', 'darkgoldenrod',\n",
    "              'darkgrey', 'darkgrey', 'darkgrey', 'darkgreen', 'darkgrey', 'darkgrey',\n",
    "              'darkgrey', 'navy', 'darkgrey']\n",
    "ax2.bar(x=mean_prod_budget_by_g_df.index, height=mean_prod_budget_by_g_df['production_budget'],\n",
    "        color=colors_ax2)\n",
    "ax2.set_xlabel('Genre')\n",
    "ax2.set_ylabel('Production Budget ($)')\n",
    "ax2.set_title('Mean Production Budget by Genre for Films Released in the Past Decade')\n",
    "ax2.ticklabel_format(axis='y', useOffset=False, style='plain')\n",
    "y = np.array([0, 20000000, 40000000, 60000000, 80000000, 100000000])\n",
    "y_ticks_labels = [\"0\", \"20M\", \"40M\", \"60M\", \"80M\", \"100M\"]\n",
    "ax2.set_yticks(y)\n",
    "ax2.set_yticklabels(y_ticks_labels)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha=\"right\");\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion on Genres\n",
    "\n",
    "Microsoft's new film studio should produce mystery films, horrors, or thrillers. These film genres have the highest ROI compared to the other genres types. In addition, they tend to be lower budget films, which could bode well for a brand new film studio that's entering show biz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directors vs. Actors Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all the relevant data into one dataframe.\n",
    "# All merges should be done with 'left merge' so that we don't lose any data.\n",
    "new_df = imdb_title_principals_df.merge(imdb_title_crew_df, on='tconst', how='left')\n",
    "new_df = new_df.merge(imdb_title_basics_df, on='tconst', how='left')\n",
    "new_df = new_df.merge(imdb_name_basics_df, on='nconst', how='left')\n",
    "master_df = new_df.merge(money_metrics_merge_ready_df, left_on='primary_title', right_on='movie', how='left')\n",
    "master_df = master_df.merge(imdb_title_ratings_df, on='tconst', how='left')\n",
    "master_df.drop(['original_title', 'birth_year', 'death_year', 'ordering', 'job', 'characters'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directors and Actors impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what kind of impact the directors have on movies, we will first take subsets of the data which have only successful movies, and compare those directors with the directors of movies that performed poorly. We can then see if it is true that you can rely on a director to make the movie successful.\n",
    "We'll then do the same for actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe that only contains movies that had an above average return on investment.\n",
    "above_average_movies = master_df[master_df['ROI'] >= money_metrics_df['ROI'].mean()]\n",
    "\n",
    "# Make a series of all the director's names, and how many of their movies were above average.\n",
    "aam_directors = above_average_movies[(above_average_movies['category'] == 'director')]['primary_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample of the 50 directors with the most movies that were succesful based on ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 50 most successful names, and the number of movies of each director for above average movies.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,12)})\n",
    "ax = sns.barplot(x=aam_directors.values.tolist()[:50], y=aam_directors.index.tolist()[:50], palette=\"Blues_d\")\n",
    "ax.set_title('Number of Above Average Movies per Director')\n",
    "ax.set_xlabel('Number of Movies')\n",
    "ax.set_ylabel('Director');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some unfamilar names there.This is because it's based on ROI, rather than gross revenue. So these guys get good bang for their buck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same series for actors and actresses.\n",
    "aam_actor_actress = above_average_movies[(above_average_movies['category'] == 'actor') | \\\n",
    "                           (above_average_movies['category'] == 'actress')]['primary_name'].value_counts()\n",
    "\n",
    "# let's look at the same sample for actors/actresses.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,12)})\n",
    "ax = sns.barplot(x=aam_actor_actress.values.tolist()[:50], y=aam_actor_actress.index.tolist()[:50], palette=\"Blues_d\")\n",
    "ax.set_title('Number of Above Average Movies per Actor/Actress')\n",
    "ax.set_xlabel('Number of Movies')\n",
    "ax.set_ylabel('Actor/Actress');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same with poor performers. But we'll only take movies that had a relatively large budget, so we eliminate the movies that, due to low budget, never had a chance to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the standard for poor performance movies at having a high budget and a low ROI.\n",
    "poor_performance_movies = master_df[(master_df['ROI'] <= money_metrics_df['ROI'].median()) & \\\n",
    "          (master_df['production_budget'] >= money_metrics_df['production_budget'].median())]\n",
    "\n",
    "# Take all the directors from the poor performing movies.\n",
    "ppm_directors = poor_performance_movies[(poor_performance_movies['category'] == 'director')]['primary_name'].value_counts()\n",
    "\n",
    "# Plot the 50 least successful names and the number of movies for each director for poor performing movies.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,12)})\n",
    "ax = sns.barplot(x=ppm_directors.values.tolist()[:50], y=ppm_directors.index.tolist()[:50], palette=\"Blues_d\")\n",
    "ax.set_title('Number of Poor Performance Movies per Director')\n",
    "ax.set_xlabel('Number of Movies')\n",
    "ax.set_ylabel('Directors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the same process for actors/actresses in poor prformance movies.\n",
    "ppm_actor_actress = poor_performance_movies[(poor_performance_movies['category'] == 'actor') | \\\n",
    "                           (poor_performance_movies['category'] == 'actress')]['primary_name'].value_counts()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,12)})\n",
    "ax = sns.barplot(x=ppm_actor_actress.values.tolist()[:50], y=ppm_actor_actress.index.tolist()[:50], palette=\"Blues_d\")\n",
    "ax.set_title('Number of Poor Performance Movies per Actor/Actress')\n",
    "ax.set_xlabel('Number of Movies')\n",
    "ax.set_ylabel('Actor/Actress');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check to see if the directors, as well as the actors, of the really successful movies also made failed movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \".intersection\" to check how many of the successful movie directors, also directed failed movies.\n",
    "num_dirs_with_good_bad = len(set(ppm_directors.index.tolist()).intersection( aam_directors.index.tolist()))\n",
    "# Check what percent of good movie directors have also directed bad movies.\n",
    "per_good_dirs_with_bad = round(num_dirs_with_good_bad/len(aam_directors)*100, 2)\n",
    "\n",
    "# Use \".intersection\" to check how many of the successful movie actors, also acted in failed movies.\n",
    "num_acts_with_good_bad = len(set(aam_actor_actress.index.tolist()).intersection(ppm_actor_actress.index.tolist()))\n",
    "# Check what percent of good movie actors have also acted in  bad movies.\n",
    "per_good_acts_with_bad = round(num_acts_with_good_bad/len(aam_actor_actress)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference between the percentages.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "ax = sns.barplot(x=['Directors', 'Actors'], y=[per_good_dirs_with_bad, per_good_acts_with_bad])\n",
    "ax.set_title('Directors vs. Actors That Have Good and Bad Films', size=20)\n",
    "ax.set_ylabel('Percent', size=20)\n",
    "ax.set_xticklabels(labels=['Directors', 'Actors'], size=15)\n",
    "ax.set_yticklabels(ax.get_yticks(), size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the director of a movie will have a bigger impact on its success than the actors.\n",
    "\n",
    "Lets see if when we filter our above_average_movies to be only big films, if the results will be any different. Lets say that we're only going to look at movies that have higher than the median production budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the condition that the production budget needs to be bigger than the median.\n",
    "bm_above_average_movies = master_df[(master_df['ROI'] >= money_metrics_df['ROI'].mean()) &\\\n",
    "                                   (master_df['production_budget'] >= money_metrics_df['production_budget'].median())]\n",
    "\n",
    "aa_big_movie_directors = bm_above_average_movies[(bm_above_average_movies['category'] == 'director')]\\\n",
    "                         ['primary_name'].value_counts()\n",
    "\n",
    "bm_aam_actor_actress = bm_above_average_movies[(bm_above_average_movies['category'] == 'actor') | \\\n",
    "                    (bm_above_average_movies['category'] == 'actress')]['primary_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \".intersection\" to check how many of the successful movie directors, also directed failed movies.\n",
    "bm_num_dirs_with_good_bad = len(set(aa_big_movie_directors.index.tolist()).intersection(ppm_directors.index.tolist()))\n",
    "# Check what percent of good movie directors have also directed bad movies.\n",
    "bm_per_good_dirs_with_bad = round(num_dirs_with_good_bad/len(aa_big_movie_directors)*100, 2)\n",
    "\n",
    "# Use \".intersection\" to check how many of the successful movie actors, also acted in failed movies.\n",
    "bm_num_acts_with_good_bad = len(set(bm_aam_actor_actress.index.tolist()).intersection(ppm_actor_actress.index.tolist()))\n",
    "# Check what percent of good movie actors have also acted in  bad movies.\n",
    "bm_per_good_acts_with_bad = round(num_acts_with_good_bad/len(bm_aam_actor_actress)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the difference between the percentages.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "ax = sns.barplot(x=['Directors', 'Actors'], y=[bm_per_good_dirs_with_bad, bm_per_good_acts_with_bad])\n",
    "ax.set_title('Directors vs. Actors That Have Good and Bad Films', size=20)\n",
    "ax.set_ylabel('Percent', size=20)\n",
    "ax.set_xticklabels(labels=['Directors', 'Actors'], size=15)\n",
    "ax.set_yticklabels(ax.get_yticks(), size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when we make take less data the differences get bigger.\n",
    "Let's flip this graph around and take the positive numbers from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage of directors/actors who were in high budget films that did well,\n",
    "# have never done films that did poorly.\n",
    "bm_per_dir_only_good = round((1-num_dirs_with_good_bad/len(aa_big_movie_directors))*100,2)\n",
    "bm_per_acts_only_good = round((1-num_acts_with_good_bad/len(bm_aam_actor_actress))*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "ax = sns.barplot(x=['Directors', 'Actors'], y=[bm_per_dir_only_good, bm_per_acts_only_good])\n",
    "ax.set_title('Directors vs. Actors That Only Have Successful Films', size=20)\n",
    "ax.set_ylabel('Percent', size=20)\n",
    "ax.set_xticklabels(labels=['Directors', 'Actors'], size=15)\n",
    "ax.set_yticklabels(ax.get_yticks(), size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude from our exploring, that it probably isn't wise to chase after the most expensive actors or actresses, since we see that many of them who have done really successful movies, have also done movies that did not perform well. This tells us, that while we know a good movie needs good acting, just because a movie has a very popular actor, the fans of this actor alone, are not going to be enough to push the needle and make it a profitable movie. For the directors, it seems that they definetely have a significant impact on the ROI. We see that from all the directors who have done successful movies, 70% of them have never done a flopped movie. So even though sometimes a good director can have a bad movie, overall money spent on a director is well spent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Runtime Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming imperative datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_title = imdb_title_basics_df.copy()\n",
    "rt_movie = rt_movie_info_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the value counts for the columns with int and float values\n",
    "imdb_title['runtime_minutes'].value_counts(), imdb_title['start_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated and show above in the Vu's Code block there are many duplicate in the IMDB Title data set, here we will check the rest of the columns in IMDB Title data set whose values are objects and not integers or floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_title['original_title'].value_counts(), imdb_title['tconst'].value_counts(), imdb_title['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see multiple duplicates withn the 'original_title\" cloumn as well and within the 'genre column'. The column genre is allowed duplicates since they are descriptive terms that are applied to movies. Want to clean this dataset up to see how they will impact the columns with int and float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing/cleaning the duplicates within original_title and primary_columns in hopes to positively impact int and float value columns \n",
    "imdb_title2 = imdb_title.drop_duplicates(subset=['original_title'], keep = 'first')\n",
    "imdb_title2 = imdb_title.drop_duplicates(subset=['primary_title'], keep = 'first')\n",
    "imdb_title2['primary_title'].value_counts(), imdb_title2['original_title'].value_counts(), imdb_title2['runtime_minutes'].value_counts(),imdb_title2['start_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that many duplicates have been removed and has impacted the amounts in the 'runtime_minutes' and 'start_year' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how the value counts of runtimes is dispersed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q_4_Imdb = imdb_title2[\"runtime_minutes\"].value_counts().quantile(1)\n",
    "q_3_Imdb = imdb_title2[\"runtime_minutes\"].value_counts().quantile(.75)\n",
    "q_2_Imdb = imdb_title2[\"runtime_minutes\"].value_counts().quantile(.5)\n",
    "q_1_Imdb = imdb_title2[\"runtime_minutes\"].value_counts().quantile(.25)\n",
    "\n",
    "q_1_Imdb , q_2_Imdb , q_3_Imdb, q_4_Imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that the delta from 75th percentile to 100th percentile is 6,404 values shows that there may be multiple occurances of just a handful of runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that will no longer be used as well as remove null values to remaining columns\n",
    "imdb_title2.dropna(subset = ['start_year'],axis = 0,  inplace = True)\n",
    "imdb_title2.dropna(subset = ['runtime_minutes'], axis = 0,  inplace = True)\n",
    "imdb_title2.drop(['primary_title'], axis =1 , inplace = True)\n",
    "imdb_title2.drop(['original_title'], axis = 1,  inplace = True)\n",
    "imdb_title2.drop(['genres'], axis = 1,  inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the most common runtime to appear in the dataset\n",
    "most_common = imdb_title2[\"runtime_minutes\"].value_counts().index[0]\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing what is the range for the time span of the data\n",
    "imdb_title2['start_year'].max(), imdb_title2['start_year'].min()\n",
    "# Spans 12 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show where the longer times are moving more towards from the most common number\n",
    "G_90_Imdb = imdb_title2.loc[(imdb_title2['runtime_minutes'] > 90), ['start_year']].mean()\n",
    "B_90_Imdb = imdb_title2.loc[(imdb_title2['runtime_minutes'] < 90), ['start_year']].mean()\n",
    "Is_90_Imdb = imdb_title2.loc[(imdb_title2['runtime_minutes'] == 90), ['start_year']].mean()\n",
    "G_90_Imdb, B_90_Imdb, Is_90_Imdb \n",
    "# Shows newer movies are tending to be longer than 90 minutes with showing the average year \n",
    "# pushing to be higher when looking at longer runtimes when looking that the mean from the\n",
    "# most common value and up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is showing a slight push towards more modern movies having more runtime lengths greater than the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now get a visual on the runtime data (IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visual on the dataset runtime values distribution we will plot the top 70 runtime values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_Runtime_Numbers = imdb_title2['runtime_minutes'].value_counts().nlargest(70)\n",
    "\n",
    "IMDB_Runtime_Numbers_Index = imdb_title2['runtime_minutes'].value_counts().index[0:70]\n",
    "fig_IMBD, ax =plt.subplots(figsize = (6,5))\n",
    "ax.bar(IMDB_Runtime_Numbers_Index ,IMDB_Runtime_Numbers , color = 'grey' )\n",
    "ax.set_facecolor('white')\n",
    "ax.set_ylabel(\"Frequency Of Runtime Numbers\")\n",
    "ax.set_title(\"Most Common Movie Lengths\")\n",
    "ax.set_xlabel(\"Runtimes in Minutes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows how often the common value of 90 minutes is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the master_df to see if having longer than normal runtimes have a ROI greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.loc[(master_df['runtime_minutes'] > 90) & (master_df['ROI'] !=0), ['ROI']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing here that even when the movie runtime is greater than industry standard of 90, overall there is a leaning towards a positive ROI greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can move onto the Rotten Tomatoes dataset to see runtimes from movies throughout history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the type of value that is within the runtime column\n",
    "type(rt_movie['runtime'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above the values in the columns are strings and must be changed into floats to be worked on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movie['runtime_minutes']=rt_movie['runtime'].str.split()\n",
    "rt_movie['runtime_minutes']=rt_movie['runtime_minutes'].str[0]\n",
    "rt_movie['runtime_minutes'] = rt_movie['runtime_minutes'].astype(float)\n",
    "# change runtime from a string to a solo float number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No column has just the year so will create one by using the values within the 'theater_date\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing just the year from theater date\n",
    "rt_movie['start_year']=rt_movie['theater_date'].str.split()\n",
    "rt_movie['start_year']=rt_movie['start_year'].str[2]\n",
    "rt_movie['start_year'] = (rt_movie['start_year']).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movie.dropna(axis = 0, subset = ['start_year'], inplace = True)\n",
    "rt_movie.dropna(axis = 0, subset = ['runtime_minutes'], inplace = True)\n",
    "# Removing any null values in the columns 'Year' and 'running_mins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing if the standard feature time of 90 is more towards current movies \n",
    "rt_movie.loc[(rt_movie['runtime_minutes'] ==90), ['start_year']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clean up the dataset by removing unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "rt_movie.drop(['runtime'], axis =1, inplace =True)\n",
    "rt_movie.drop(['studio'],axis = 1, inplace = True)\n",
    "rt_movie.drop(['currency'],axis = 1, inplace = True)\n",
    "rt_movie.drop(['writer'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dispersion of the runtimes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_4_rt = rt_movie[\"runtime_minutes\"].value_counts().quantile(1)\n",
    "q_3_rt = rt_movie[\"runtime_minutes\"].value_counts().quantile(.75)\n",
    "q_2_rt = rt_movie[\"runtime_minutes\"].value_counts().quantile(.5)\n",
    "q_1_rt = rt_movie[\"runtime_minutes\"].value_counts().quantile(.25)\n",
    "q_1_rt , q_2_rt, q_3_rt, q_4_rt\n",
    "#Less of a delta here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking normal runtimes for newer movies\n",
    "print(rt_movie.loc[(rt_movie['start_year'] >=2015),['runtime_minutes']].mean())\n",
    "print(rt_movie.loc[(rt_movie['start_year'] >=2015),['runtime_minutes']].max())\n",
    "print(rt_movie.loc[(rt_movie['start_year'] >=2015),['runtime_minutes']].min())\n",
    "# rt_movie.loc[(rt_movie['Year'] >=2015),['running_mins']]\n",
    "#Ave 16 mins longer than mode of data set\n",
    "# Max length 148 min\n",
    "# Min legnth 80\n",
    "#Showing More Modern Movies are longer than the feature length Standard\n",
    "# Modern movies being greater in this case being 2015 and present "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now get a visual on the runtime data (RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot a chart the get a visual on runtime with a datset that has a longer time history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_70_RT_Indexes = rt_movie['runtime_minutes'].value_counts().index[:70]\n",
    "Top_70_RT = rt_movie['runtime_minutes'].value_counts().nlargest(70)\n",
    "#grabbbing 70 most common runtimes from rotten tomatoes file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_rt , ax = plt.subplots(figsize = (8,6))\n",
    "ax.bar(Top_70_RT_Indexes , Top_70_RT, color = 'grey' )\n",
    "ax.set_ylabel(\"Frequency Of Runtime Numbers\")\n",
    "ax.set_xlabel(\"Run Times in Minutes\")\n",
    "ax.set_title('Most Common Movie Lengths')\n",
    "ax.set_facecolor('white')\n",
    "ax.legend(['As Per Rotten Tomatoes']);\n",
    "# plot showing top 70 most common runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing grouping in the 90 minute mark but with a right skew to longer movie times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering Values within Rotten Tomatoes File Where only looking at frequncy runtime within the past 10 years\n",
    "# just like the IMDB dataset\n",
    "P10_Years_RT = rt_movie.loc[(rt_movie['start_year'] >=2011),['runtime_minutes']].value_counts().nlargest(50)\n",
    "ten = P10_Years_RT.index[0:50]\n",
    "# way to turn multi-index into float values for the x column\n",
    "P10_Index = ten.get_level_values(0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph shoiwing most frequent runtimes of the past 10 years rotten tomatoes graph\n",
    "fig_Rotten_Modern , ax= plt.subplots(figsize = (8,6))\n",
    "ax.bar(P10_Index, P10_Years_RT, color = 'grey')\n",
    "ax.set_xlabel('Runtimes in Minutes ')\n",
    "ax.set_facecolor('white')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Past 10 Years')\n",
    "ax.legend(['As Per Rotten Tomatoes']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Past 10 years fall along IMDB's past 10 years as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a far better visual on how runtimes have been either increasing or decreasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_1 = rt_movie.loc[rt_movie['start_year'] < 1941, ['runtime_minutes']].mode()\n",
    "mode_2 = rt_movie.loc[(rt_movie['start_year'] >= 1941) & (rt_movie['start_year'] <1961), ['runtime_minutes']].mode()\n",
    "mode_3 = rt_movie.loc[(rt_movie['start_year'] >= 1961) & (rt_movie['start_year'] <1981), ['runtime_minutes']].mode()\n",
    "mode_4 = rt_movie.loc[(rt_movie['start_year'] >= 1981) & (rt_movie['start_year'] <2001), ['runtime_minutes']].mode()\n",
    "mode_5 = rt_movie.loc[(rt_movie['start_year'] >= 2001) & (rt_movie['start_year'] <2011), ['runtime_minutes']].mode()\n",
    "mode_6 = rt_movie.loc[(rt_movie['start_year'] >= 2011) , ['runtime_minutes']].mode()\n",
    "mode_7 = rt_movie.loc[(rt_movie['start_year'] == 2018) , ['runtime_minutes']].mode()\n",
    "median_1 = rt_movie.loc[rt_movie['start_year'] < 1941, ['runtime_minutes']].median()\n",
    "median_2 = rt_movie.loc[(rt_movie['start_year'] >= 1941) & (rt_movie['start_year'] <1961), ['runtime_minutes']].median()\n",
    "median_3 = rt_movie.loc[(rt_movie['start_year'] >= 1961) & (rt_movie['start_year'] <1981), ['runtime_minutes']].median()\n",
    "median_4 = rt_movie.loc[(rt_movie['start_year'] >= 1981) & (rt_movie['start_year'] <2001), ['runtime_minutes']].median()\n",
    "median_5 = rt_movie.loc[(rt_movie['start_year'] >= 2001) & (rt_movie['start_year'] <2011), ['runtime_minutes']].median()\n",
    "median_6 = rt_movie.loc[(rt_movie['start_year'] >= 2011) , ['runtime_minutes']].median()\n",
    "median_7 = rt_movie.loc[(rt_movie['start_year'] == 2018) , ['runtime_minutes']].median()\n",
    "mean_1 = rt_movie.loc[rt_movie['start_year'] < 1941, ['runtime_minutes']].mean()\n",
    "mean_2 = rt_movie.loc[(rt_movie['start_year'] >= 1941) & (rt_movie['start_year'] <1961), ['runtime_minutes']].mean()\n",
    "mean_3 = rt_movie.loc[(rt_movie['start_year'] >= 1961) & (rt_movie['start_year'] <1981), ['runtime_minutes']].mean()\n",
    "mean_4 = rt_movie.loc[(rt_movie['start_year'] >= 1981) & (rt_movie['start_year'] <2001), ['runtime_minutes']].mean()\n",
    "mean_5 = rt_movie.loc[(rt_movie['start_year'] >= 2001) & (rt_movie['start_year'] <2011), ['runtime_minutes']].mean()\n",
    "mean_6 = rt_movie.loc[(rt_movie['start_year'] >= 2011) , ['runtime_minutes']].mean()\n",
    "mean_7 = rt_movie.loc[(rt_movie['start_year'] == 2018) , ['runtime_minutes']].mean()\n",
    "print(mode_1, mode_2, mode_3, mode_4, mode_5, mode_6, mode_7)\n",
    "print(mean_1, mean_2, mean_3, mean_4, mean_5, mean_6, mean_7)\n",
    "print(median_1, median_2, median_3, median_4, median_5, median_6, median_7)\n",
    "#modes being in order   80, 90 , 95, 95, 102, 100, 105 with mode_5 and mode_7 having multiple modes therefore took the average of them\n",
    "#medians being 94, 98, 104, 103, 102, 104, 107\n",
    "#means 100, 102, 110, 107, 104, 107, 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_values = [80, 90, 95, 95, 102, 100, 105]\n",
    "Time = ['1921', '1941', '1961', '1981', '2001', '2011', '2018' ]\n",
    "median_values = [94,98,104,103,102,104,107]\n",
    "mean_values = [100, 102, 110, 107, 104, 107, 111]\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.lineplot(Time, mode_values)\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.lineplot(Time, median_values)\n",
    "sns.lineplot(Time, mean_values)\n",
    "plt.legend(['Mode of Movie Length', 'Median of Time Points' , 'Mean of Time Points'])\n",
    "plt.ylabel('Run Times')\n",
    "plt.xlabel('Timeline')\n",
    "plt.title('The Gradual Increase Of Movie Runtimes Through History ');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have gotten down here to the graph above it shows how overtime the runtimes are going longer than the industry standard. With the industry standard being implemented back in the 1920's to make sure movies weren't overbearing to the public. But now that movies are getting longer and straying from the 90 minute mark on the upperside it is safe to say that following the trend of breaking the industry standard is a safe bet because it can allow the studio to not be constricted to this 90 minute mark and possibly limiting what can be put in the film. Most films that do go beyond the 90 minute runtime are still on the greater than one ROI value so following the trend is less risky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Monthly Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common staple of the Hollywood schedule is the seasonal trends. October for horror, summer for action, awards season bait in the winter. But how does this shake out as far as ROI goes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing the mmmr_df from above\n",
    "months_and_years_df = money_metrics_merge_ready_df.copy()\n",
    "\n",
    "#dropping the zeros gross films\n",
    "#This may be be very generous to our ROI data later, but we have to assume a literal 0 return is \n",
    "#more likely bad data in than a literal 0 revenue. \n",
    "#these film makers have parents, that at least would provide some box office.\n",
    "months_and_years_df = months_and_years_df[months_and_years_df['worldwide_gross'] != 0].copy()\n",
    "\n",
    "# Removing Deep Throat on principle. It's numbers were apparently pumped by mob involvement and\n",
    "# apparently the lead talent was coerced into performing, which, ick.\n",
    "# Also it's messing up my graphs.\n",
    "months_and_years_df = months_and_years_df.sort_values('ROI', ascending = False)\n",
    "months_and_years_df = months_and_years_df.drop([5745])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Block of useful funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Making a series in the data frame that will let us sort our months in order.\n",
    "months_and_years_df['alphaMonths']=months_and_years_df['Month']\n",
    "\n",
    "#A brute function to cycle through\n",
    "def alphamonth(month):\n",
    "    if month == 'Jan':\n",
    "        amonth = '01 - January'\n",
    "    elif month == 'Feb':\n",
    "        amonth = '02 - February'\n",
    "    elif month == 'Mar':\n",
    "        amonth = '03 - March'\n",
    "    elif month == 'Apr':\n",
    "        amonth = '04 - April'\n",
    "    elif month == 'May':\n",
    "        amonth = '05 - May'\n",
    "    elif month == 'Jun':\n",
    "        amonth = '06 - June'\n",
    "    elif month == 'Jul':\n",
    "        amonth = '07 - July'\n",
    "    elif month == 'Aug':\n",
    "        amonth = '08 - August'\n",
    "    elif month == 'Sep':\n",
    "        amonth = '09 - September'\n",
    "    elif month == 'Oct':\n",
    "        amonth = '10 - October'\n",
    "    elif month == 'Nov':\n",
    "        amonth = '11 - November'\n",
    "    elif month == 'Dec':\n",
    "        amonth = '12 - December'\n",
    "    return amonth\n",
    "\n",
    "#And .map ing that through the df\n",
    "months_and_years_df['alphaMonths']=months_and_years_df['alphaMonths'].map(alphamonth) \n",
    "\n",
    "#functions to pull years, months out of the data set. Can also be used for other column types?\n",
    "\n",
    "#for now, use int types for the year\n",
    "def year_iso (df, year):\n",
    "    output_df = df[df['Year'] == year]\n",
    "    return output_df\n",
    "\n",
    "\n",
    "#for now, with money_metrics_df, use three letter abreviations for months\n",
    "def month_iso (df, month):\n",
    "    output_df = df[df['Month'] == month]\n",
    "    return output_df\n",
    "\n",
    "\n",
    "#A function to take any dataframe e.g. an isolated year, and get a monthly breakdown\n",
    "def run_the_months(df):\n",
    "    month_list= ['January','February','March', 'April', 'May','June','July','August',\n",
    "                           'September','October','November','December']\n",
    "    df_list=[]\n",
    "    for i in month_list:\n",
    "        month_abrev= i[0:3]\n",
    "        data_frame_to_list = month_iso(df , month_abrev)\n",
    "        df_list.append(data_frame_to_list)\n",
    "        output_dict = dict(zip(month_list, df_list))    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation revealed that early data was too sparse and noise to analyse well, and likely had less bearing on modern trends. A new dataframe for 1980 and beyond was created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's push this to be a post 1970's display. A lot of the pre-70s data is too thin on the ground.\n",
    "#Most execs' oldest movie memories probably goes back to about the 80's\n",
    "\n",
    "post_70s_df =  months_and_years_df[months_and_years_df['Year'] >= 1980]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Using seaborn to get a pretty graph of ROI by month. Also, automates a lot of previous work. C'est la vie.\n",
    "### Default estimator is mean, which works for us.\n",
    "\n",
    "figure(figsize=(18, 12))\n",
    "sns.set_style(\"ticks\")\n",
    "plt.title('Monthly Mean ROI, 1980 and Beyond')\n",
    "sns.barplot(data = post_70s_df.sort_values('alphaMonths'), x = 'alphaMonths', y = 'ROI')\n",
    "plt.xlabel('Month');\n",
    "\n",
    "#May and July stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There clearly are trends here. May and July stand out as creat times to release. December and January seem less promising. But let's dig a bit deeper. The Hollywood seasons are often associated with different genres. Let's break down our top genres by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Let's do by month AND genre\n",
    "\n",
    "#dropping all other columns to make merging simpler\n",
    "genre_barebones = all_g_df[['movie','genre']]\n",
    "\n",
    "genre_month_merge = post_70s_df.merge(genre_barebones, how= 'inner', left_on = 'movie', right_on= 'movie')\n",
    "\n",
    "#Plotting the top three genres by month\n",
    "top_three_genre = genre_month_merge[(genre_month_merge['genre'] == 'Horror') | \n",
    "                                    (genre_month_merge['genre'] == 'Mystery') \n",
    "                                    | (genre_month_merge['genre'] == 'Thriller')]\n",
    "\n",
    "#sorting by alphamonths to make the incoming graph sort Jan-Dec\n",
    "top_three_genre.sort_values('alphaMonths', inplace = True)\n",
    "\n",
    "\n",
    "###And plot it all!\n",
    "sns.catplot(data = top_three_genre, x = 'alphaMonths', y = 'ROI', hue=\"genre\", \n",
    "            kind=\"bar\",height=10, ci= None, aspect =1.6 )\n",
    "sns.set_style(\"ticks\")\n",
    "plt.title('Monthly Mean ROI by Genre, 1980 and Beyond')\n",
    "plt.xlabel('Month');\n",
    "#I'm trimming out the error bars, but this sucker is *noisy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find some surprises! January, it turns out, is a pretty good time to release a horror film! Thrillers do well in April.\n",
    "\n",
    "Film release dates should take genre into consideration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
